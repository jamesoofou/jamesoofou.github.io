<!DOCTYPE html>
<html>
  <head>
    <title>jim's web site</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="./style.css" id="main-stylesheet">
  </head>
  <body>
    <p style="font-family: Courier, monospace; font-size: 22px;">
    <a href="./index.html" style="font-family: Courier monospace; font-size: 22px;">&#x27F5;</a>
    
<div style="font-family: Courier, monospace; font-size: 22px;">
    <h1>The State of OpenAI</h1>

<p>Notes on <a href="https://youtu.be/ngDCxlZcecw?si=nqjD-8Qw16sH28Xk">Sam, Jakub, and Wojciech on the future of OpenAI with audience Q&A</a>.</p>

<p>OpenAI is excited by (a) third-party apps that use AI and (b) AI accelerating science.</p>

<p>OpenAI's research focus is on scaling up deep learning with the goal of fundamentally changing the pace of scientific research, especially AI research. OpenAI is "a research organisation working on automating research".</p>

<p>OpenAI believes that time horizons will continue to extend rapidly, due to a combination of algorithmic innovation and the scaling of existing architectures. Progress is expected to come in particular from scaling test-time compute where OpenAI sees "orders and orders of magnitude to go".</p>

<img src="openai1.png">

<p>OpenAI's internal goal to automate AI research by 2028 March is interesting. By naive extrapolation we'd be at METR-80 of 30h at that point. That's a METR-50 of 150h. Hmm. 150h = maybe 3 months of hard work for a human (actually, Jim Fund thinks that it likely means extremely superhuman insight but in any case...). They probably just did a naive extrapolation and didn't account for hyperexponentaility. Jesus. Reality will be a lot crazier.</p>

<img src="openai2.png">

<p>"We have quite strong expectations for our next models, we expect quite rapid progress over the next couple months and a year".</p>

<p>OK</p>

<p>[Paraphrase:] The way this works is we develop a lot of pieces, each a hard-won victory, and we know that when we put them together we will have something quite impressive, we're able to predict this fairly well. Part of our goal today is to say that we have a lot of those pieces" (He's talking about "a tremendously important step forward in capability")</p>

<p>OK</p>

<p>So, OpenAI is promising a big leap forward within the next 6 months, likely sooner. A bunch of pieces coming together. I'm guessing this looks like large advances in general due to a more universal approach to verification, and great improvements in mathematics, and great improvements in science. It will also look like great imrovements in coding agents and, likely, the release of a vibe-coding tool.</p>

<p>Contrast this to Alphabet's Sundar's statement on Gemini 3.0:</p>

<blockquote>Thanks, Justin. The first on the pace of frontier model research and development. Look, I think two things are both simultaneously true. I'm incredibly impressed by the pace at which the teams are executing and the pace at which we are improving these models. It also is true, at the same time, that each of the prior models you're trying to get better over is now getting more and more capable. I think both the pace is increasing, but sometimes we are taking the time to put out a notably improved model. I think that may take slightly longer. I do think the underlying pace is phenomenal to see. I'm excited about our Gemini 3.0 release later this year.</blockquote>

<p>Not clear how much to read into Sundar's statement. Maybe he's holding his cards close to his chest. Maybe Gemini 3.0 really was not a big enough jump at first so they had to keep iterating on it. In which case it sounds like OA is ahead on the LLM front. Not really clear. One thing is for sure: being long Microsoft (which owns 27% of OpenAI) seems like a good idea.</p>



</div>

  </body>
</html>