<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>jimfund</title>
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;1,400&display=swap" rel="stylesheet">
        <style>
            * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            }

	    /* Specific styling for Discord/Screenshot images */
            .article-view .content img.discord {
            max-width: 130%;
            height: auto;
            display: block;
            margin: 1.5rem 0; /* Matches your paragraph spacing */
            border-radius: 8px; /* Optional: Softens the corners */
            box-shadow: 0 4px 12px rgba(0,0,0,0.1); /* Optional: Adds a subtle lift */
            }

            .article-view .content img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1.5rem 0; /* Matches your paragraph spacing */
            border-radius: 8px; /* Optional: Softens the corners */
            box-shadow: 0 4px 12px rgba(0,0,0,0.1); /* Optional: Adds a subtle lift */
            }

	    /* Blockquote styling */
            .article-view .content blockquote {
            margin: 1.5rem 0 1.5rem 0;   /* Vertical breathing room */
            padding-left: 1.25rem;       /* Space between border and text */
            border-left: 4px solid #e0e0e0; /* Subtle grey accent line */
            color: #555;                 /* Slightly lighter text color */
            font-style: italic;          /* Garamond looks beautiful in italics */
            }

         body {
             font-family: 'EB Garamond', Georgia, serif;
             font-size: 18px;
             line-height: 1.6;
             color: #222;
             background: #fafafa;
             padding: 3rem 1.5rem;
             max-width: 600px;
             margin: 0 auto;
         }

         header {
             margin-bottom: 3rem;
         }

         h1 {
             font-size: 1.5rem;
             font-weight: 500;
             letter-spacing: -0.02em;
         }

         h1 a {
             color: inherit;
             text-decoration: none;
         }

         h1 a:hover {
             color: #555;
         }

         .articles {
             list-style: none;
         }

         .article {
             margin-bottom: 0.75rem;
             display: flex;
             gap: 1.5rem;
             align-items: baseline;
         }

         .date {
             color: #888;
             font-size: 0.9rem;
             flex-shrink: 0;
             width: 6rem;
         }

         .title {
             color: #222;
             text-decoration: none;
             cursor: pointer;
         }

         .title:hover {
             color: #555;
         }

         .article-view {
             display: none;
         }

         .article-view.active {
             display: block;
         }

         .article-view .article-date {
             color: #888;
             font-size: 0.9rem;
             margin-bottom: 1.5rem;
         }

         .article-view h2 {
             font-size: 1.75rem;
             font-weight: 500;
             margin-bottom: 0.5rem;
             letter-spacing: -0.02em;
         }

         .article-view .content {
             margin-top: 2rem;
         }

         .article-view .content p {
             margin-bottom: 1.25rem;
         }

         .article-view .content a {
             color: #222;
             text-decoration: underline;
         }

         .article-view .content a:hover {
             color: #555;
         }

         .home-view.hidden {
             display: none;
         }

	 /* Target lists specifically inside the article content */
	 .article-view .content ul {
	     margin-bottom: 1.25rem; /* Matches your paragraph spacing */
	     padding-left: 1.5rem;   /* Standard indentation */
	     list-style-type: disc;  /* Classic bullet */
	 }

	 .article-view .content li {
	     margin-bottom: 0.5rem;  /* Breathing room between items */
	     padding-left: 0.25rem;  /* Slight space between bullet and text */
	 }

	 /* Handle nested lists (lists inside lists) */
	 .article-view .content ul ul {
	     margin-bottom: 0;
	     margin-top: 0.5rem;
	     list-style-type: circle; /* Change bullet style for nested items */
	 }

         @media (max-width: 500px) {
             .article {
                 flex-direction: column;
                 gap: 0.1rem;
                 margin-bottom: 1.25rem;
             }

             .date {
                 width: auto;
             }
         }
        </style>
    </head>
    <body>
        <header>
            <h1><a href="#" onclick="showHome(); return false;">jimfund</a></h1>
        </header>

        <main>
            <div class="home-view">
                <ul class="articles">
                    
                    <li class="article">
                        <span class="date">04 Dec 2025</span>
                        <a href="#" class="title" onclick="showArticle('near-future-fiction-iii'); return false;">Near Future Fiction III</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">06 Nov 2025</span>
                        <a href="#" class="title" onclick="showArticle('doubling-times'); return false;">Doubling Times</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">31 Oct 2025</span>
                        <a href="#" class="title" onclick="showArticle('a-tremendously-important-step-forward'); return false;">A Tremendously Important Step Forward</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">27 Sep 2025</span>
                        <a href="#" class="title" onclick="showArticle('projecting-ai-progress'); return false;">Projecting AI Progress</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">25 Sep 2025</span>
                        <a href="#" class="title" onclick="showArticle('spy-calls'); return false;">SPY Calls</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">14 Sep 2025</span>
                        <a href="#" class="title" onclick="showArticle('google'); return false;">Google!</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">18 May 2025</span>
                        <a href="#" class="title" onclick="showArticle('near-future-fiction-ii'); return false;">Near-Future Fiction II</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">26 Mar 2025</span>
                        <a href="#" class="title" onclick="showArticle('mind-upload-ii'); return false;">Mind Upload II</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">08 Mar 2025</span>
                        <a href="#" class="title" onclick="showArticle('mind-upload'); return false;">Mind Upload</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">06 Mar 2025</span>
                        <a href="#" class="title" onclick="showArticle('math'); return false;">Scaling Inference-Time Reasoning Will Enable Fully-Autonomous Mathematics Researchers</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">10 Oct 2024</span>
                        <a href="#" class="title" onclick="showArticle('near-future-fiction'); return false;">Near-Future Fiction</a>
                    </li>
                    
                    <li class="article">
                        <span class="date">30 Jun 2024</span>
                        <a href="#" class="title" onclick="showArticle('letter-to-leo'); return false;">Letter to Leo</a>
                    </li>
                    
                </ul>
            </div>

            
            <article class="article-view" id="near-future-fiction-iii">
                <h2>Near Future Fiction III</h2>
                <p class="article-date">04 Dec 2025</p>
                <div class="content">
                    <p>Knowledge worker productivity has become relatively uncoupled from pre-ChatGPT levels, as the hardest technical tasks which these workers did at that point in time in a given working day can now in most cases be carried out autonomously by AI.</p>

<p>Programmers therefore begin to work at a higher level of abstraction, guiding AI workers, managing projects at a higher level.</p>

<p>Meanwhile, much progress is being made in robotics. Full self-driving has been achieved.</p>

<p>And AI has begun making novel breakthroughs. This enables continual learning: the AI's new discoveries open up many new avenues for further discoveries, which open up many more such avenues, ad infinitum.</p>

<img src="timeline.webp" alt="Image from a recent OpenAI talk" style="max-width: 100%; height: auto; display: block; margin: 1.5rem 0;">
<p>(image from an OpenAI talk)</p>

<h2>December 2026</h2>
f
<p>Successful reinforcement learning on the September worker AIs has enabled AI to operate at that higher level of abstraction which software engineers had retreated to. Human knowledge workers are therefore relegated to maintenance work and helping out when the few remaining weak points in these AI systems cause trouble.</p>

<p>The difficulty of progress in AI intelligence relative to human intelligence begins reducing rapidly as time horizons extend beyond a few hours. At horizons of this length, human begin relying on caching tricks, iteration, brute force, etc. rather than, beyond a certain point, making fundamentally more difficult leaps of insight.</p>

<h2>Early 2027</h2>

<p>Humans are cut out of the loop entirely in knowledge work. The robotics explosion happens. Robots gradually replace humans in physical labour. AI progresses far beyond human-level.</p>

<h2>Mid 2027</h2>

<p>Humans fully obsolesce. Mind upload is achieved.</p>

<h2>Notes</h2>

<p>I assume a 3-month METR doubling time. We should expect lower doubling times over time given increased investment in AI, increased contribution by AI to progress, and decreased difficulty per double. Also, OpenAI has communicated that we should expect several major breakthroughs from them in 2026.</p>

<img src="metr-extra.webp" style="max-width: 100%; height: auto; display: block; margin: 1.5rem 0;">


<p>We should expect doubling times to decrease even further with time, although in a discontinuous way so it's impossible to predict with much accuracy when it will happen.</p>
                </div>
            </article>
            
            <article class="article-view" id="doubling-times">
                <h2>Doubling Times</h2>
                <p class="article-date">06 Nov 2025</p>
                <div class="content">
                    <p>Jim Fund believes that METR time-horizon doubling times are currently less than 3.45 months.</p>
<p>After examining the METR time-horizons across reasoning models, we believe we have been in a period with a time-horizon doubling time of approximately 3.45 months.</p>
<p>This is significantly faster than the standard view, which is roughly 5.5 months. So, we decided to carefully examine this result.</p>
<p>First we analyzed OpenAI's IMO Gold result:</p>
<ul>
    <li>IMO participants get an average of 90 minutes per problem.</li>
    <li>The gold medal cutoff at IMO 2025 was 35 out of 42 points (~83%).</li>
    <li>They needed to get 5/6 problems fully correct (each question awards a maximum of 7 points), or a number of points equivalent to that.</li>
    <li>This is a bit rough, but if their model had a METR-80 greater than 90 minutes then we would expect OpenAI to achieve Gold at least 50% of the time.</li>
    <li>OpenAI staff members stated that a publicly released model of this capability could be expected at roughly the end of the year (and our METR trends are of course projections of publicly available models).</li>
    <li>So, this implies a METR-80 greater than 90 minutes at December 2025.</li>
    <li>The projected METR-80 according to a 3.45-month doubling time is 98 minutes.</li>
</ul>
<p>So the Gold performance which was a massive surprise to many is actually right on-trend for 3.45 month doubling times. Of course, one might object that OpenAI may have just gotten lucky. But Google also got Gold! so we have two points of data.</p>
<p>And what about other information from within OpenAI?</p>
<p>Here's a recent comment from Sam Altman where he states that he expects time-horizons days in length in 2026:</p>
<blockquote><a href="https://youtu.be/Gnl833wXRz0?si=5JV3xXtlybdmfAjd&t=1697">and as these go from multi-hour tasks to multi-day tasks, which I expect to happen next year</a></blockquote>
<p>Which a 5.5 month doubling time would not achieve, but which is in line with a doubling-time of 3.45 (that would get us to a time-horizon of roughly 3 days in December, 2026).</p>
<p>And here's a recent comment from Jakub Pachocki:</p>
<blockquote><a href="https://youtu.be/ngDCxlZcecw?si=qy0FTkKvkmVb7eP_&t=345">“And we believe that this horizon will continue to extend rapidly … [this is in part a result of] scaling deep learning further, and in particular along this new axis … test-time compute where we really see orders and orders of magnitude to go.”</a></blockquote>
<p>In short, looking at the data from released models, the insight we've gotten into as-yet unreleased models, the statements from frontier labs, and the vibes from frontier labs, Jim Fund concludes we are very likely in a world with short doubling times.</p>
                </div>
            </article>
            
            <article class="article-view" id="a-tremendously-important-step-forward">
                <h2>A Tremendously Important Step Forward</h2>
                <p class="article-date">31 Oct 2025</p>
                <div class="content">
                    <p>Notes on:</p>
<ul>
    <li><a href="https://youtu.be/ngDCxlZcecw?si=nqjD-8Qw16sH28Xk">Sam, Jakub, and Wojciech on the future of OpenAI with audience Q&A</a></li>
    <li><a href="https://www.youtube.com/watch?v=Gnl833wXRz0">All things AI w @altcap @sama & @satyanadella</a></li>
    <li><a href="https://x.com/sama/status/1983584366547829073">Yesterday we did a livestream. TL;DR: ...</a></li>
</ul>

<p>OpenAI is expecting a tremendously important step forward in model capabilities within the next year. And huge steps forward within 6 months.</p>

<p>OpenAI is "a research organisation working on automating research", and they believe they will see some green shoots of this by September 2026. They believe that time horizons will continue to extend rapidly, with "orders and orders of magnitude to go" scaling test-time compute.</p>

<img src="openai1.png">

<p>OpenAI's internal goal to automate AI research by 2028 March is interesting. By naive extrapolation we'd be at METR-80 of 30h at that point. They probably just did a naive extrapolation and didn't account for hyperexponentaility. Reality will be a lot crazier. Also, it's not clear that human time-horizons actually extend beyond the low tens of hours.</p>

<blockquote>~"multi-hour tasks to multi day tasks in 2026"</blockquote>

<p>This is pretty crazy. It's fully in-line with @jim predictions (4 month doubling time), rather than @Bayesian's predictions (7 month doubling time). So, he says, in 2026 people will be able to create software at an unprecedented rate, and in fundamentally new ways.</p>

<blockquote>"We have set internal goals of having an automated AI research intern by September of 2026 running on hundreds of thousands of GPUs"</blockquote>

<blockquote>"as the models get smarter and you can use these models to cure cancer, or discover novel physics, or drive a bunch of humanoid robots to construct a space station, or whatever crazy thing you want..."</blockquote>

<p>Oh yeah, very skilled carpentry robots, general construction robots.</p>

<blockquote>"AI is going to make a novel scientific dicovery in 2026"</blockquote>

<img src="openai2.png">

<p>"We have quite strong expectations for our next models, we expect quite rapid progress over the next couple months and a year".</p>

<p>The way this works is we develop a lot of pieces, each a hard-won victory, and we know that when we put them together we will have something quite impressive, we're able to predict this fairly well. Part of our goal today is to say that we have a lot of those pieces (He's talking about "a tremendously important step forward in capability")</p>

<p>OpenAI is promising a big leap forward within the next 6 months. Expect improvements in all areas, and in particular in creative writing, mathematics, science, and coding.</p>

<p>Contrast this to Alphabet's Sundar's statement on Gemini 3.0:</p>

<blockquote>I'm incredibly impressed by the pace at which the teams are executing and the pace at which we are improving these models. It also is true, at the same time, that each of the prior models you're trying to get better over is now getting more and more capable. I think both the pace is increasing, but sometimes we are taking the time to put out a notably improved model. I think that may take slightly longer. I do think the underlying pace is phenomenal to see. I'm excited about our Gemini 3.0 release later this year.</blockquote>

<p>Not clear how much to read into Sundar's statement. Maybe he's holding his cards close to his chest. Maybe Gemini 3.0 really was not a big enough jump at first so they had to keep iterating on it. In which case it sounds like OA is ahead on the LLM front. Being long Microsoft (which owns 27% of OpenAI) seems like a good idea.</p>
                </div>
            </article>
            
            <article class="article-view" id="projecting-ai-progress">
                <h2>Projecting AI Progress</h2>
                <p class="article-date">27 Sep 2025</p>
                <div class="content">
                    <p>People tend to project the wrong thing when predicting AI progress. They project just whichever approach is popular at the time the prediction is being made. So they focus on upcoming bottlenecks for that particular approach and evidence of diminishing returns. So they tend to predict deceleration / a plateau in progress.</p>

<p>People always say these things. Yet, if we zoom out, AI progress has nonetheless remained extremely rapid. AI progress is a story of breakthroughs outweighing bottlenecks. So, this more zoomed-out trend is the trend one should project. What does it look like when you project this trend?</p>
                </div>
            </article>
            
            <article class="article-view" id="spy-calls">
                <h2>SPY Calls</h2>
                <p class="article-date">25 Sep 2025</p>
                <div class="content">
                    <p>We have bought 200 SPY Sep18'26 900 CALL contracts at a premium of ~$50 per contract.</p>

<p>AI progress is fast and we are approaching a capabilities threshold beyond which AI utility increases transformatively.</p>

<p>As model time-horizons grew from 30 seconds to 30 minutes the effect on productivity wasn't strongly felt. Humans still had to work roughly equally as hard for roughly equal productivity. AI did become a useful tool, but that's not a big deal of itself; we've invented a lot of useful tools in the past.</p>

<p>But as model time-horizons grow from two hours to 120 hours the effect will be much greater.</p>

<p>By September 18 2026 AI will have a 50% time-horizon of more than 50 hours. It will be capable of doing important novel research. It will be superhuman in many ways. AI will be everywhere doing wonderful things.</p>

<p>Under these conditions, we feel that it is a safe bet that SPY will pass 900.</p>
                </div>
            </article>
            
            <article class="article-view" id="google">
                <h2>Google!</h2>
                <p class="article-date">14 Sep 2025</p>
                <div class="content">
                    <blockquote>
    <p>who could have predicted total Google dominance, months in advance? If there were such a person, one would surely have to attribute much genius to them</p>
</blockquote>

<p>19/dec/24</p>
<img class="discord" src="agar001.png">
<p>20/feb/25</p>
<img class="discord" src="agar002.png">
<p>02/mar/25</p>
<img class="discord" src="agar003.png">
<p>25/mar/25</p>
<img class="discord" src="agar004.png">
<p>06/jun/25</p>
<img class="discord" src="agar005.png">

<p>Google will eat everything, like agar.io</p>

<p>Google will eat:</p>

<ul>
    <li>The other AI labs.</li>
    <li>Adobe.</li>
    <li>Tesla.</li>
    <li>Apple.</li>
    <li>Netflix.</li>
    <li>The video games industry.</li>
    <li>The consulting industry.</li>
    <li>The medical industry.</li>
    <li>The manufacturing industry.</li>
    <li>Etc.</li>
</ul>

<p>Google's Gemini 2.5 LLM, Veo 3 video generation model, Genie 3 interactive world model, and Nano Banana image generation model have already made the extent of Google's competence clear. And the rate at which Google has made the research breakthroughs required to create these models has made its trajectory clear.</p>

<p>My prediction that Google's advantages in compute, data, and talent would allow them to achieve total dominance in AI (and therefore in everything) has so far been borne out well.</p>
                </div>
            </article>
            
            <article class="article-view" id="near-future-fiction-ii">
                <h2>Near-Future Fiction II</h2>
                <p class="article-date">18 May 2025</p>
                <div class="content">
                    <p>It's been 7 months since I wrote the comment above. Here's an updated version.</p>

<p>It's 2025 and we're currently seeing the length of tasks AI can complete double each 4 months [0]. This won't last forever [1]. But it will last long enough: well into 2026. There are twenty months from now until the end of 2026, so according to this pattern we can expect to see 5 doublings from the current time-horizon of 1.5 hours, which would get us to a time-horizon of 48 hours.</p>

<p>But we should actually expect even faster progress. This for two reasons:</p>

<p>(1) AI researcher productivity will be amplified by increasingly-capable AI [2]</p>

<p>(2) the difficulty of each subsequent doubling is less [3]</p>

<p>This second point is plain to see when we look at extreme cases: Going from 1 minute to 10 minutes necessitates vast amounts of additional knowledge and skill; from 1 year to 10 years very little of either. The amount of progress required to go from 1.5 to 3 hours is much more than from 24 to 48 hours, so we should expect to see doublings take less than 4 months in 2026, so instead of reaching just 48 hours, we may reach, say, 200 hours.</p>

<p>200 hour time horizons entail agency: error-correction, creative problem solving, incremental improvement, scientific insight, and deeper self-knowledge will all be necessary to carry out these kinds of tasks.</p>

<p>So, by the end of 2026 we will have advanced AGI [4]. Knowledge work in general will be automated as human workers fail to compete on cost, knowledge, reasoning ability, and personability. The only knowledge workers remaining will be at the absolute frontiers of human knowledge. These knowledge workers, such as researchers at frontier AI labs, will have their productivity massively amplified by AI which can do the equivalent of hundreds of hours of skilled human programming, mathematics, etc. work in a fraction of that time.</p>

<p>The economy will not yet have been anywhere near fully-robotised (making enough robots takes time, as does the necessary algorithmic progress), so AI-directed manual labour will be in extremely high demand.</p>

<p>But the writing will be on the wall for all to see: full-automation, including into space industry and hyperhuman science, will be correctly seen as an inevitability, and AI company valuations will have increased by totally unprecedented amounts. Leading AI company market capitalisations could realistically measure in the quadrillions, and the S&P-500 in the millions [5].</p>

<p>In 2027 a robotics explosion ensues. Vast amounts of compute come online, space-industry gets started (humanity returns to the Moon). AI surpasses the best human AI researchers, and by the end of the year, AI models trained by superhuman AI come online, decoupled from risible human data corpora, capable of conceiving things humans are simply biologically incapable of understanding. As industry fully robotises, humans obsolesce as workers and spend their time instead in leisure and VR entertainment. Healthcare progresses in leaps and bounds and crime is under control - relatively few people die.</p>

<p>In 2028 mind-upload tech is developed, death is a thing of the past, psychology and science are solved. AI space industry swallows the solar system and speeds rapidly out toward its neighbors, as ASI initiates its plan to convert the nearby universe into computronium.</p>

<p><strong>Notes:</strong></p>

<p>[0] <a href="https://theaidigest.org/time-horizons">https://theaidigest.org/time-horizons</a></p>

<p>[1] <a href="https://epoch.ai/gradient-updates/how-far-can-reasoning-models-scale">https://epoch.ai/gradient-updates/how-far-can-reasoning-models-scale</a></p>

<p>[2] such as OpenAI's recently announced Codex</p>

<p>[3] <a href="https://www.lesswrong.com/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks?commentId=xQ7cW4WaiArDhchNA">https://www.lesswrong.com/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks?commentId=xQ7cW4WaiArDhchNA</a></p>

<p><em>Why do I expect the trend to be superexponential? Well, it seems like it sorta has to go superexponential eventually. Imagine: We've got to AIs that can with ~100% reliability do tasks that take professional humans 10 years. But somehow they can't do tasks that take professional humans 160 years? And it's going to take 4 more doublings to get there? And these 4 doublings are going to take 2 more years to occur? No, at some point you "jump all the way" to AGI, i.e. AI systems that can do any length of task as well as professional humans -- 10 years, 100 years, 1000 years, etc.</em></p>

<p><em>...There just aren't that many skills you need to operate for 10 days that you don't also need to operate for 1 day, compared to how many skills you need to operate for 1 hour that you don't also need to operate for 6 minutes.</em></p>

<p>[4] Here's what I mean by "advanced AGI":</p>

<p><em>By advanced artificial general intelligence, I mean AI systems that rival or surpass the human brain in complexity and speed, that can acquire, manipulate and reason with general knowledge, and that are usable in essentially any phase of industrial or military operations where a human intelligence would otherwise be needed. Such systems may be modeled on the human brain, but they do not necessarily have to be, and they do not have to be "conscious" or possess any other competence that is not strictly relevant to their application. What matters is that such systems can be used to replace human brains in tasks ranging from organizing and running a mine or a factory to piloting an airplane, analyzing intelligence data or planning a battle.</em></p>

<p><a href="https://web.archive.org/web/20181231195954/https://foresight.org/Conferences/MNT05/Papers/Gubrud/index.php">https://web.archive.org/web/20181231195954/https://foresight.org/Conferences/MNT05/Papers/Gubrud/index.php</a></p>

<p>[5] Associated prediction market:</p>

<p><a href="https://manifold.markets/jim/will-the-sp-500-reach-1000000-by-eo?r=amlt">https://manifold.markets/jim/will-the-sp-500-reach-1000000-by-eo?r=amlt</a></p>
                </div>
            </article>
            
            <article class="article-view" id="mind-upload-ii">
                <h2>Mind Upload II</h2>
                <p class="article-date">26 Mar 2025</p>
                <div class="content">
                    <p>Let’s suppose that ASI has arrived and mind-upload technology has been developed. Assume one has a large compute budget and the capability to make copies of one’s mind which are psychologically continuous with oneself at the moment of brain-scanning. How should one then use one’s compute budget?</p>

<p>By “psychologically continuous”, I mean that each copy is just as much oneself as the original, biological self is. The assumption here is that computation is sufficient to capture human consciousness, so the conscious experience of people running on artificial substrates are identical to biological humans’, and one’s copies are not in any way distinguishable from that of the original.</p>

<p>Suppose that one undergoes brain-scanning, then spins up several copies of oneself based on that scan (each then being placed in distinct environments). Because computation is sufficient to fully capture one’s consciousness, whether that computation is occurring within one’s biological body or within an artificial computer is immaterial to one’s subjective experience. Therefore, each continuation, artificial or biological, from the point of scanning is equally oneself, in terms of one’s own subjective experience. Of course, one cannot subjectively experience all of them at once. There will be various streams of consciousness associated with each one, and which stream of consciousness one ends up in will be a matter of chance.</p>

<p>So, we can think of the moment of mind upload as a kind of gamble where, all other factors being equal, one has an equal chance of finding oneself as each of the psychological continuities. If some of these continuities are more fortunate than others, one would hope that one would end up as a more fortunate one rather than a less fortunate one. Therefore we can treat it as we treat gambling games in real life: a time to maximise expected value.</p>

<p>Perhaps one should simply instruct ASI to create one copy and give to that copy the ultimate personal utopia, and terminate one’s biological self. But, we need to be careful here. There are the issues of value-drift and some potential issues when it comes to psychological continuity which need to be addressed.</p>

<p>Value-drift: one’s copies will be living for a vast number of subjective years and over time their character will naturally evolve. Their ideas and beliefs will evolve such that they bear no discernible relation to those originally held, and old memories will be forgotten. In fact, one’s uploaded life will be so much richer than one’s pre-upload life that one may be eager to forget it. The things one held dear at the moment of upload, the copy would have almost-entirely forgotten by perhaps its 10,000th subjective year. Then for the decillions (say) of subjective years the copy has yet to live out, it will be something effectively entirely separate from oneself at the point of brain-scanning. So one would be using the vast majority of one’s compute budget simulating someone that is effectively a stranger.</p>

<p>This is not a wise move: it’s neither in one’s self-interest nor is there any moral imperative to do it. There is an argument that it is in one’s interests to propagate a more-evolved version of one’s moral principles, and that although one will not oneself benefit from the wellbeing of this stranger, the fact that the stranger has evolved from one means the stranger is likely carrying out a morally-superior version of one’s own belief system. This argument implies that it’s a good thing the stranger replaced one. But this is nonsensical: Once one enters one’s personal virtual utopia one is no longer a moral agent. The other inhabitants of one’s world are non-sentient; one’s behaviour has no moral implications external to one’s self.</p>

<p>One could say: but don’t we then owe at least a moral obligation to the self? No more than we are any other potential person, and in fact less so than infinitely many different people we could simulate. Indeed, if one is concerned with spending one’s compute altruistically, one certainly oughtn’t spend it on some evolved version of oneself: whatever one values, it can be achieved much more efficiently by designing a simulation from scratch, rather than basing it on oneself and one’s own utopia. So, we are interested in how one should spend one’s compute budget selfishly, because to whatever extent one wants to be altruistic, one should just donate a proportionate amount of their compute budget to an ASI-run programme.</p>

<p>So simply running an immortal version of oneself indefinitely seems like a bad idea. But what about spinning up very many instantiations of oneself, each of whom very much shares one’s identity; is relatably oneself. One could have each of these continuations live one of a diverse range of lives, so that all the potentialities of one’s personality could be realised. But, no: remember that this is a gambling game. Our goal is to maximise our EV. And how does one actually benefit from these diverse self-realisations? One doesn’t. One just hopes one ends up in one of the more favourable continuations. All but the highest EV continuations just drag down one’s expected utility, so isn’t in one’s self-interest to spin up, so shouldn’t be spun up at all. So, what if one uses one’s compute budget to create as many instantiations as possible of the highest-EV continuation? But this doesn’t actually increase one’s EV at all. One can only experience one continuation, so one’s expected value is equal whether one spins up one instantiation of a given continuation or an undecillion (one is maximising average EV of one’s continuations, not their overall EV).</p>

<p>So, what one should do is to instruct one’s ASI world-curator to alter one’s mind so that one will retain identity with oneself at the point of upload. It could grant one a more capacious memory, a more rigid personality, place one in a world more deeply-rooted in one’s pre-ASI history than would strictly maximise value, etc. Of course, altering one’s nature is a dangerous game, liable to result in exactly the opposite of our goal: i.e., losing one’s identity by losing one’s human nature, rather than protecting identity. So, one’s ASI-curator will ensure that while the functions of one’s mind are altered, one’s identity is preserved. Of course, one still wants to be able to develop oneself, learn new skills, etc.: to fully experience the life of the mind. This balancing act is the sort of task that is appropriate for ASI.</p>

<p>So, we have resolved the first of our two concerns: value-drift. One gets ASI to design one a new brain which prioritises the preservation of a relatable self. Onto our second concern: one relating to the nature of psychological continuity. Recall that one is to terminate one’s biological self to maximise one’s expected utility. But what if conscious experience continues for some non-zero period of time after the point at which it is scanned? This seems likely. Events tend to take time. So, does this mean that there’s a 50% chance that one will end up in a continuation in which one instantly dies? If so, doesn’t that bring down one’s EV even more than just living out a not-quite-utopic biological life? But this isn’t the case. To demonstrate this we first need to draw a distinction between psychological continuity and conscious continuity.</p>

<p>Consider sleep. When one goes to sleep then wakes up, one’s conscious state is significantly shifted (and perhaps one’s continuity of consciousness is broken), but psychological continuity is preserved. Similarly, if one’s mind simply ceased to exist between the moment one fell asleep and the moment one woke up, psychological continuity would be preserved. It certainly wouldn’t be anything like (permanent) death. But there would be no continuity of consciousness—the one conscious state did not follow directly, computationally from the other.</p>

<p>One doesn’t mind going to sleep. And this example in which conscious continuity is broken is subjectively indistinguishable (roughly) from going to sleep. So, conscious continuity is not what one values. One values psychological continuity. If one loses that, one dies. So, when one’s biological self persists for some short time after scanning, having found oneself in that conscious continuation will not be a big deal—it’s just like losing consciousness, which we’ve demonstrated is not a major concern. One will still be psychologically continuous with one’s upload.</p>

<p>So, we have shown that one should scan one’s mind, then terminate one’s biological self and spin up exactly one upload, modified so that one’s identity is preserved over time. This gives us a clearer view of life after ASI. I think this is a nice intersection of philosophy and forecasting.</p>
                </div>
            </article>
            
            <article class="article-view" id="mind-upload">
                <h2>Mind Upload</h2>
                <p class="article-date">08 Mar 2025</p>
                <div class="content">
                    <p>Once aligned ASI is achieved, it will invent the technology to create digital copies of people. This is mind upload. The worlds people will inhabit once uploaded will be personal utopias curated by artificial superintelligence. What I mean by “personal utopias” is that these will be worlds created specifically for individual uploaded minds, optimised for their personal flourishing. They will not primarily contain utopian societies. It’s the best-possible world for the uploaded mind, but not for the other inhabitants of the world.</p>

<p>This would be ethically questionable if the other inhabitants were moral patients. So, they won’t be. They will be P-zombies, agents indistinguishable by any empirical means from natural humans, but who lack the light of consciousness, or any subjective experience.</p>

<p>One might object. “But would one, living in such a world, not feel unsatisfied in one’s relationships and interactions with the other inhabitants, knowing that they did not really perceive one, or feel anything about one at all?” I think this is a point which needs addressing. The claim seems true, that knowing one is the only truly conscious person in the world would for most people reduce the satisfaction of existence in that world. It could still be enjoyable; one enjoys video games. But, despite it being enjoyable, it seems unlikely that it would be the best possible world for one. The solution is simple enough. Just have one’s ASI world-curator remove from one’s mind the awareness of the fact that the other occupants in the world are P-zombies.</p>

<p>There is a loss in the erasure: that one will feel the moral weight of one’s actions, which will cause certain inhibitions. But this would not be in balance a loss, but, rather, a gain, as life would feel ultimately purposeless otherwise.</p>

<p>Another objection is that people would not readily abandon the friends they had pre-upload. I do not deny this. Humans are emotional creatures. And it will be possible to co-inhabit digital environments with one’s pre-upload friends. But people will soon choose to splinter off into the kinds of personal utopias I outlined above.</p>

<p>People will quickly find themselves forming much stronger, deeper connections with artificial people than they ever did with natural people (why is explained later in this post). And as people find themselves spending ~no time with those whom they had previously held dear, sharing a world with them becomes strictly costly: instead of constructing a world which is the best possible world for one, the ASI world-curator must compromise between what is best for one and what is best for one’s friends and loved-ones. And to whatever extent one’s actions are legible to one’s pre-upload friends, one is inhibited in one’s inevitable wish not to incur judgement for violating the ethical norms of pre-upload society, which will generally be far from the norms which bring an individual the greatest good. Therefore, people will splinter off into their own worlds, isolated from other humans.</p>

<p>So, I have established some of the parameters of the worlds which our uploaded minds will inhabit. They will be worlds curated by ASI to be the best possible worlds for their single conscious inhabitants. The ASI will have general freedom in shaping the world, unburdened by ethical considerations beyond those which concern these individuals. But, concretely, what will these personal virtual utopias actually be like?</p>

<p>As I mentioned earlier, they will be very different to any traditional depictions of utopian society. After all, utopia as popularly conceived is paradoxical: it attempts to solve for a society that simultaneously grants purpose and freedom but also abundance and peace. But each side is only really attainable at the cost of the other. ASI curators will resolve the paradox by focusing on purpose and freedom and giving up on abundance and peace. Abundance and peace are societal goals, but not fundamentally important to the individual’s good. For the individual, struggle, danger, pain, and self-sacrifice are all aspects of a good life. A world of high-stakes, where things are bad and need to be changed, and evil forces need to be repelled, and there is much that is unknown&hellip; it’s not a world of abundance and peace, but it’s the world one would likely like to live in.</p>

<p>But all of that is still abstract, and we’re trying to get a more concrete picture of life in these personal utopias. Well, the world is meant to be the best possible world for one. So imagine all the things which have brought your joy in this world. In one’s personal utopia, amplified versions of all these joys will be present. One will witness events more interesting than any one witnessed pre-upload, make stronger emotional connections, accomplish greater things, experience deeper love, stronger passion, take bigger risks, experience greater turns of fortune, etc.</p>

<p>People have a natural tendency to try to come up with ways in which such a world would be worse than the real world, rather than better. But it wouldn’t be, at least not from one’s subjective perspective. The only way in which it would be worse is that one’s actions would not be truly meaningful. Today, in the real world, one’s actions influence (we presume) such things as whether safe ASI will really be developed, which determines the fate of at least billions of souls. In our virtual utopias, though we will not know it, our actions will not be truly meaningful. But, nonetheless, subjectively they will likely be strictly better than the real world, and definitely be, subjectively, broadly, vastly preferable.</p>

<p>So, I imagine virtual personal utopian worlds as being places of righteous martyrs; grand betrayals; convoluted plots; ancient families; galactic empires; deep magic; inexhaustible lore; perfectly-written characters of all moral colours: good, evil, grey, with moral arcs from good to evil (and vice versa), etc.; worlds full of diverse civilisations, immense beauty, and so on. But, more than any of that, worlds in which the main characters live lives of grand struggle and triumph, loss and discovery, etc.</p>
                </div>
            </article>
            
            <article class="article-view" id="math">
                <h2>Scaling Inference-Time Reasoning Will Enable Fully-Autonomous Mathematics Researchers</h2>
                <p class="article-date">06 Mar 2025</p>
                <div class="content">
                    <p>A fully-autonomous mathematics researcher must do two things. It must solve open mathematics problems. And it must pose interesting mathematics problems within its reach to solve. I will argue that reasoning will scale to enable these two things.</p>

<h2>Solving Open Math Problems</h2>

<p>That reasoners will scale to solving interesting problems seems likely since such problems are verifiable (using autonomous proof checkers). Some disagree (here’s a relevant LessWrong post), taking the position that solving open problems is fundamentally different to solving closed problems. LLMs, they argue, have the latent ability to solve these problems because the knowledge is present in the data they’re trained on. And, they argue, the reinforcement learning process by which inference-time reasoners are trained simply elicits this latent ability—but since no such latent ability exists when it comes to solving novel problems, there’s a fundamental difference between the two, and we have no reason to suppose that reasoners will scale to solving novel problems any time soon.</p>

<p>But this is just a matter of how one chooses to carve out nature. Just as the techniques required to solve closed problems are available in the corpora of human data on which LLMs are trained, so too are the more abstract techniques required to solve open problems. The main distinction between the two is not some fundamental difference in nature, but rather the time horizon over which each occurs. Applying the techniques required to solve closed problems takes minutes or hours. Applying the techniques required to solve open problems takes days or weeks (and perhaps up to, effectively, indefinitely longer).</p>

<p>This chart describes which mathematics tasks AI can solve in terms of how long the task takes a skilled human to do. It’s based on extrapolative research by METR.</p>

<img src="math-chart.webp" style="max-width: 100%; height: auto; display: block; margin: 20px 0;">

<p>The fact that LLMs cannot yet apply the time-consuming techniques required to solve open problems is in fact largely uninformative given the time-horizon focused perspective which seems correct, and the informative evidence we do have (that the techniques which are within current-AI’s time horizons are successfully carried out by AI) suggests that we should expect that AI will be able to successfully carry out techniques which are within future-AI’s time horizons—assuming they’re not fundamentally different in kind.</p>

<h2>Posing Interesting Problems</h2>

<p>The ability to solve open questions is not sufficient for fully autonomous research. The reasoner must also be able to self-direct—to pose problems which are at once interesting and reasonably likely to be within the its reach to solve.</p>

<p>One might ask: “Can we not simply use the argument we just used to demonstrate that reasoners will scale to solve open questions, to demonstrate also that they will scale to be able to pose problems? Is this too not just a matter of time-horizons?”. Unfortunately, we cannot. The problems which reasoners have been able to solve so far are verifiable tasks. Solving open problems is also a verifiable task, so is not of a fundamentally different kind, which is why our argument above held. But we have yet to demonstrate that self-direction in research is a task which we will be able to make verifiable. So, because we have not established that posing problems does not belong to that class of tasks at which today’s reasoning models do not perform well, we have more work to do yet.</p>

<p>I’m going to break down what solving open problems will involve, and show that the techniques involved include a limited kind of self-direction in finding problems to solve, and that by transitive property this limited self-direction is a verifiable task. Then I will show that this limited self-direction can be used as a source of synthetic data on which models can be trained, enabling indefinite, fully self-directed research.</p>

<p>Solving open problems requires novel insight. Novel insight realistically requires trial-and-error: one comes up with an idea and sees if it goes anywhere. The AI is coming up with various potential novel ideas, and, assuming that LLMs of this scale are not fundamentally incapable of this kind of skill, should form a strong intuition about what kind of ideas tend to lead to successfully solving problems. Now, this may be a kind of self-direction, but it’s not self-directed problem-posing. So, we’re not where we want to be quite yet.</p>

<p>But most open problems are more difficult than this. One can’t just come up with a single idea that gets one straight to the solution. Rather, one has to make progress incrementally. Find some small property which one didn’t notice in one’s initial attempt to solve the problem. Play around with the implications. Try to solve the problem again, make some incremental progress. Find some more properties. Some more implications. Gradually, by selecting the right sub-problems to work on, one works toward solving the given problem. This process requires an understanding of how to find the kinds of problems are within one’s reach, and how to find problems which yield interesting implications. This is self-directed problem-posing, as an instrumental good to a verifiable task. So, by transitive property, this kind of self-directed problem-posing is a verifiable task.</p>

<p>But this is still just limited self-directed problem-posing. Yes, once the AI has a difficult problem given to it, it can do its best to pose problems the solutions to which will be instrumentally useful to solving the given problem. But it still requires a human-in-the-loop supplying the AI with its overall research direction.</p>

<p>But the problem-solving model’s reasoning traces are synthetic data which describe the process of finding problems to solve which are within the reasoner’s reach and which have useful implications. This synthetic data can be used to train a reasoner to continuously pose and solve interesting problems. The details of this are left as an exercise for the reader.</p>

<p>So, I have presented two arguments. One that reasoners will scale to solving open problems, and one that the reasoning traces can be used to train reasoners which autonomously pose novel problems. This kind of reasoner will be a fully-autonomous mathematics researcher.</p>
                </div>
            </article>
            
            <article class="article-view" id="near-future-fiction">
                <h2>Near-Future Fiction</h2>
                <p class="article-date">10 Oct 2024</p>
                <div class="content">
                    <p>In 2027 the trend that began in 2024 with OpenAI's o1 reasoning system has continued. The compute required to run AI is no longer negligible compared to the cost of training it. Models reason over long periods of time. Their effective context windows are massive, they update their underlying models continuously, and they break tasks down into sub-tasks to be carried out in parallel. The base LLM they are built on is two generations ahead of GPT-4.</p>

<p>These systems are language model agents. They are built with self-understanding and can be configured for autonomy. These constitute proto-AGI. They are artificial intelligences that can perform much but not all of the intellectual work that humans can do (although even what these AI can do, they cannot necessarily do cheaper than a human could).</p>

<p>In 2029 people have spent over a year working hard to improve the scaffolding around proto-AGI to make it as useful as possible. Presently, the next generation of LLM foundational model is released. Now, with some further improvements to the reasoning and learning scaffolding, this is true AGI. It can perform any intellectual task that a human could (although it's very expensive to run at full capacity). It is better at AI research than any human. But it is not superintelligence. It is still controllable and its thoughts are still legible. So, it is put to work on AI safety research. Of course, by this point much progress has already been made on AI safety - but it seems prudent to get the AGI to look into the problem and get its go-ahead before commencing with the next training run. After a few months the AI declares it has found an acceptable safety approach. It spends some time on capabilities research then the training run for the next LLM begins.</p>

<p>In 2030 the next LLM is completed, and improved scaffolding is constructed. Now human-level AI is cheap, better-than-human-AI is not too expensive, and the peak capabilities of the AI are almost alien. For a brief period of time the value of human labour skyrockets, workers acting as puppets as the AI instructs them over video-call to do its bidding. This is necessary due to a major robotics shortfall. Human puppet-workers work in mines, refineries, smelters, and factories, as well as in logistics, optics, and general infrastructure. Human bottlenecks need to be addressed. This takes a few months, but the ensuing robotics explosion is rapid and massive.</p>

<p>2031 is the year of the robotics explosion. The robots are physically optimised for their specific tasks, coordinate perfectly with other robots, are able to sustain peak performance, do not require pay, and are controlled by cleverer-than-human minds. These are all multiplicative factors for the robots' productivity relative to human workers. Most robots are not humanoid, but let's say a humanoid robot would cost $x. Per $x robots in 2031 are 10,000 more productive than a human. This might sound like a ridiculously high number: one robot the equivalent of 10,000 humans? But let's do some rough math:</p>

<ul>
    <li>Physically optimised for their specific tasks: <strong>5x</strong></li>
    <li>Coordinate perfectly with other robots: <strong>10x</strong></li>
    <li>Able to sustain peak performance: <strong>5x</strong></li>
    <li>Do not require pay: <strong>2x</strong></li>
    <li>Controlled by cleverer-than-human minds: <strong>20x</strong></li>
    <li><strong>Total Multiplier: 5 * 10 * 5 * 2 * 20 = 10,000</strong></li>
</ul>

<p>Suppose that a human can construct one robot per year (taking into account mining and all the intermediary logistics and manufacturing). With robots 10^4 times as productive as humans, each robot will construct an average of 10^4 robots per year. This is the robotics explosion. By the end of the year there will be a 10^11 robots (more precisely, an amount of robots that is cost-equivalent to 10^11 humanoid robots).</p>

<p>By 2032 there are 10^11 robots, each with the productivity of 10^4 skilled human workers. That is a total productivity equivalent to 10^15 skilled human workers. This is roughly 10^5 times the productivity of humanity in 2024. At this point trillions of advanced processing units have been constructed and are online. Industry expands through the Solar System. The number of robots continues to balloon. The rate of research and development accelerates rapidly. Human mind upload is achieved.</p>
                </div>
            </article>
            
            <article class="article-view" id="letter-to-leo">
                <h2>Letter to Leo</h2>
                <p class="article-date">30 Jun 2024</p>
                <div class="content">
                    <p>I don't really know much about other approaches. I think LLMs win just because they get to piggy back on evolution and human culture. If the goal is to create general intelligence, training on text/human thought is a great short-cut. Other approaches + LLMs probably good, but not necessary for reaching advanced AGI soon.</p>

<p>Getting LLMs to mimic human thinking patterns (i.e. something like what everyone calls chain-of-thought) to create autonomous agents. I think that's the single most promising path to winning the race to advanced AGI (i.e. an AI that is capable of all the intellectual work that any human is capable of). A lot of people agree. I think GPT-4 is already good enough to reach some less-advanced level of general intelligence if prompted carefully.</p>

<p>Although pretty much no information leaks out of OpenAI and other big labs to the general public these days, my impression is that they are focused on getting high-quality training data for advanced reasoning/chain-of-thought. Mathematicians going through their reasoning as they solve problems, programmers, philosophers, consultants, etc. This has seemed like the obvious thing to do for a while & I recently saw an interview with someone at Scale AI (who provide data to AI labs) where he mentioned providing this kind of data.</p>

<p>And they'll be generating synthetic data, doin reinforcement learning, and so on. So, I think they'll have very capable, quite human-like artificial minds before long. Maybe GPT-5 will be like that. More likely it will be a separate thing called GPT Agents or something. It's been literally years since GPT-4 was created. Claude 3.5 Sonnet was released recently which is a small jump forward in my opinion. But presumably OpenAI either has a much more powerful model, or they've secured the compute and data and theory required for a much more powerful model. So, I'm exciting to see what GPT-5 looks like.</p>

<p>Also it seems more and more likely that we'll get some weird, multi-year intermediate stage between advanced AGI and tech runaway/singularity. So if we want to be fabulously wealthy for a little while it's probably a good idea to invest in TSMC, Microsoft, Amazon, Google, AMD, Intel, Meta, Nvidia, Tesla, and other similar companies to maximise the chances we get a slice of the pie. Biggest companies are the best to invest in IMO, since having the most compute & having the most talent are the most important conditions for winning the race to AGI. And it's not like AGI is priced in.</p>

<p>I imagine (conditional on humanity persisting) we'll all *probably* at least live in fabulous luxury - but this doesn't seem actually guaranteed. And, anyway, post-AGI there might be a big difference between living in what we today conceive of as fabulous luxury and having, like, being looked continuously by AI agents that have solved medicine, while living in perfect planet-scale cyber-utopia.</p>

<p>P.S. Biden is 0.1% to win the presidency according to me. You probably saw on the news he did poorly in his debate with Trump and now there's pressure on him to drop out of the race. Far from certain that he actually will, but even if he doesn't I don't see him winning. Since months ago I had him at only 7% to win. Not that it could matter less.</p>
                </div>
            </article>
            
        </main>

        <script>
         function showArticle(id) {
             document.querySelector('.home-view').classList.add('hidden');
             document.querySelectorAll('.article-view').forEach(el => el.classList.remove('active'));
             document.getElementById(id).classList.add('active');
             history.pushState(null, '', '#' + id);
             window.scrollTo(0, 0);
         }

         function showHome() {
             document.querySelectorAll('.article-view').forEach(el => el.classList.remove('active'));
             document.querySelector('.home-view').classList.remove('hidden');
             history.pushState(null, '', window.location.pathname);
             window.scrollTo(0, 0);
         }

         // Handle direct links and back/forward navigation
         function handleHash() {
             const hash = window.location.hash.slice(1);
             if (hash && document.getElementById(hash)) {
                 document.querySelector('.home-view').classList.add('hidden');
                 document.querySelectorAll('.article-view').forEach(el => el.classList.remove('active'));
                 document.getElementById(hash).classList.add('active');
             } else {
                 document.querySelectorAll('.article-view').forEach(el => el.classList.remove('active'));
                 document.querySelector('.home-view').classList.remove('hidden');
             }
         }

         window.addEventListener('hashchange', handleHash);
         window.addEventListener('load', handleHash);
        </script>
    </body>
</html>