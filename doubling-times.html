<!DOCTYPE html>
<html>
  <head>
    <title>jim's web site</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="./style.css" id="main-stylesheet">
  </head>
  <body>
    <p style="font-family: Courier, monospace; font-size: 22px;">
    <a href="./index.html" style="font-family: Courier monospace; font-size: 22px;">&#x27F5;</a>
    
<div style="font-family: Courier, monospace; font-size: 22px;">
    <h1>Doubling times</h1>
    <p>Looking at the improvement in METR time-horizons across reasoning models Jim Fund believes we have been in a period with a time-horizon doubling time of aproximately 3.45 months.</p>
    <p>This is significantly faster than the standard view, which is roughly 5.5 months. So, we decided to carefully examine this result.</p>
    <p>First we analyzed OpenAI's IMO Gold result:</p>
    <ul>
	<li>IMO participants get an average of 90 minutes per problem.</li>
	<li>The gold medal cutoff at IMO 2025 was 35 out of 42 points (~83%).</li>
	<li>They needed to get 5/6 problems fully correct (each question awards a maximum of 7 points), or a number of points equivalent to that.</li>
	<li>This is a bit rough, but if their model had a METR-80 greater than 90 minutes then we would expect OpenAI to achieve Gold at least 50% of the time.</li>
	<li>OpenAI staff members stated that a publicly released model of this capability could be expected at roughly the end of the year (and our METR trends are of course projections of publicly available models).</li>
	<li>So, this implies a METR-80 greater than 90 minutes at December 2025.</li>
	<li>The projected METR-80 according to a 3.45-month doubling time is 98 minutes.</li>
    </ul>
    <p>So the Gold performance which was a massive surprise to many is actually right on-trend for 3.45 month doubling times. Of course, one might object that OpenAI may have just gotten lucky. But Google also got Gold! so we have two points of data.</p>
    <p>And what about other information from within OpenAI?</p>
    <p>Here's a recent comment from Sam Altman where he states that he expects time-horizons days in length in 2026:</p>
    <blockquote><a href="https://youtu.be/Gnl833wXRz0?si=5JV3xXtlybdmfAjd&t=1697">and as these go from multi-hour tasks to multi-day tasks, which I expect to happen next year</a></blockquote>
    <p>Which a 5.5 month doubling time would not achieve, but which is in line with a doubling-time of 3.45 (that would get us to a time-horizon of roughly 3 days in December, 2026).</p>
    <p>And here's a recent comment from Jakub Pachocki:</p>
    <blockquote><a href="https://youtu.be/ngDCxlZcecw?si=qy0FTkKvkmVb7eP_&t=345">“And we believe that this horizon will continue to extend rapidly … [this in part a result of] scaling deep learning further, and in particular along this new axis … test-time compute where we really see orders and orders of magnitude to go.”</a></blockquote>
    <p>In short, looking at the data from released models, the insight we've gotten into as-yet unreleased models, the statements from frontier labs, and the vibes from frontier labs, Jim Fund concludes we are very likely in a world with short doubling times.</p>
</div>

  </body>
</html>