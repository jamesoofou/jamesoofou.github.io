<!DOCTYPE html>
<html>
  <head>
    <title>jim's web site</title>
    <link rel="stylesheet" href="./style.css" id="main-stylesheet">
  </head>
  <body>
    <a href="./home.html">
  <img src="./castle-static.gif"
       data-gif="./tower.gif" 
       class="hover-gif"
       alt="home">
</a>

<script>
  // Optimized hover GIF handling
  (function () {
      const gifs = document.querySelectorAll('.hover-gif');
      if (!gifs.length) return;

      const dataMap = new WeakMap();

      gifs.forEach((img) => {	  const staticSrc = img.getAttribute('src');
				  const gifSrc = img.dataset.gif;
				  if (!gifSrc) return;

				  dataMap.set(img, { staticSrc, gifSrc, preloaded: false });
			    });

      function bustCache(url) {
	  const sep = url.includes('?') ? '&' : '?';
	  return url + sep + 't=' + Date.now();
      }

      function preloadGif(entry) {
	  if (entry.preloaded) return;
	  const pre = new Image();
	  pre.decoding = 'async';
	  pre.src = bustCache(entry.gifSrc);
	  entry.preloaded = true;
      }

      document.addEventListener('mouseover', onEnter, { passive: true, capture: true });
      document.addEventListener('mouseout', onLeave, { passive: true, capture: true });

      function isEnteringFromOutside(e) {
	  const related = e.relatedTarget;
	  return !related || (related !== e.target && !e.target.contains(related));
      }

      function isLeavingToOutside(e) {
	  const related = e.relatedTarget;
	  return !related || (related !== e.target && !e.target.contains(related));
      }

      function onEnter(e) {
	  const img = e.target;
	  if (!(img instanceof HTMLImageElement)) return;
	  if (!img.classList || !img.classList.contains('hover-gif')) return;
	  if (!isEnteringFromOutside(e)) return;

	  const entry = dataMap.get(img);
	  if (!entry) return;

	  preloadGif(entry);
	  img.src = bustCache(entry.gifSrc);
      }

      function onLeave(e) {
	  const img = e.target;
	  if (!(img instanceof HTMLImageElement)) return;
	  if (!img.classList || !img.classList.contains('hover-gif')) return;
	  if (!isLeavingToOutside(e)) return;

	  const entry = dataMap.get(img);
	  if (!entry) return;

	  img.src = entry.staticSrc;
      }

      if ('IntersectionObserver' in window) {
	  const io = new IntersectionObserver(
	      (entries) => {
		  for (const { isIntersecting, target } of entries) {
		      if (!isIntersecting) continue;
		      const entry = dataMap.get(target);
		      if (!entry) continue;
		      preloadGif(entry);
		      io.unobserve(target);
		  }
	      },
	      { rootMargin: '200px' }
	  );

	  gifs.forEach((img) => io.observe(img));
      }
  })();
</script>    
    
<h1>Library</h1>
<ul>
  <li>
    <ul>
      <li>Aug 1 2025</li>
      <li><a href="https://arxiv.org/pdf/2303.10130">GPTs are GPTs: An Early Look at the Labor Market Impact Potential
	  of Large Language Models</a></li>
      <li>Modeling economic impact of LLMs</li>
      <details>
	<summary>notes</summary>
	<blockquote>General-purpose technologies (e.g. printing, the steam engine) are characterized by widespread proliferation, continuous improvement, and the generation of complementary innovations (Bresnahan and Trajtenberg, 1995; Lipsey et al., 2005).</blockquote>
	<p> OK, so LLMs will be characterised by widespread proliferation (i.e. extreme adoption in multiple sectors), continuous improvement, and the generation of complementary innovations. "Complementary innovations"... such as?</p>
	<blockquote>We use the ONET 27.2 database (ONET, 2023), which contains information on 1,016 occupations, including their respective Detailed Work Activities (DWAs) and tasks. A DWA is a comprehensive action that is part of completing task, such as "Study scripts to determine project requirements." A task, on the other hand, is an occupation-specific unit of work that may be associated with zero, one, or multiple DWAs.</blockquote>
	<p>OK, this is really cool. I can potentially copy their methodology but taking into account capabilities advances in LLMs?</p>
	<p>I should read (Lipsey et al., 2005)</p>
	<p>So, I should see how open each industry sector is to LLM-automation, then look at which portion each sector makes up of the SPX,... but then what? What does this tell us? Well, if <i>n</i>% of a sector is automatable, that tells us a little. But what really matters is to what extent it can be accelerated / sublimated / multiplied / etc., that is, to what extent LLMs can increase the amount and quality of output in that sector. So, I would get an LLM to go through the task list, and evaluate each task for the extent to which an LLM with an 80% time-horizon in the hundreds of hours could increase the quantity, quality according to some rubric.</p>
	<p>It seems to me that one of the main reasosn white-colar jobs will be automated before blue-color jobs is that there simply is not enough robotic machinary to replace billions of human workers yet, whereas there is perhaps, or soon will be enough compute to replace white-colar workers (as more compute comes online and the cost of a given level of intelligence decreases). There will be a robotics explosion, but it will lag behind a little, I suspect.</p>
	<p>Rather than going through the ONET tasks, which seems unlikely to be productive when attempting to predict the impact of >100 hour time horizons on the value of the SPX, I think we should sample 20 companies at random from the SPX and look deeply at how this kind of full-automation of white-colar work would affect them.<p>
	<p>Also, logically, AI will rarely <i>equal</i> human work output. It will either surpass it or fall short of it. Few roles in these companies are binary in nature, where they are either accomplished successfully or non accomplished. So we have to look at how much employee costs can be reduced, how much productivity will increase, how important employee costs are to the valuation of the company...</p>
	<p>Also, we should look at what's been published about how a growing economy increases valuations of companies regardless of their own efficiency gains. that is, will there be some feedback relationship where each company becoming more productive and profitable pushes up other companies, which also are more efficient, and which therefore push also the others up?</p>
      </details>
      
    </ul>
  </li>
  <li>
    <ul>
      <li>Aug 1 2025</li>
      <li><a href="https://jwmason.org/wp-content/uploads/2021/01/Coyle-GDP-ch.-1.pdf">GDP: A Brief But Affectionate History (Chapter One)</a></li>
      <li>Explains what GDP is</li>
    </ul>
  </li>
  <li>
    <ul>
      <li>Aug 2 2025</li>
      <li><a href="https://epoch.ai/gradient-updates/quantifying-the-algorithmic-improvement-from-reasoning-models">Quantifying the algorithmic improvement from reasoning models</a></li>
      <details>
	<summary>notes</summary>
	reasoning pushed us ahead 2 years, another advancement or two like this and we're golden. The "by 2030 or bust" school of thought about arrival of AGI seems foolish given that even if frontier compute growth slows to a standstill these innovations will likely still be a regular occurrence.
      </details>
    </ul>
</ul>

  </body>
</html>